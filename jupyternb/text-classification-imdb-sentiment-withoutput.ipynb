{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1660d77-3952-4bc5-91b2-035b1e615c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from: https://www.tensorflow.org/tutorials/keras/text_classification\n",
    "\n",
    "# all the prerequisites\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "print(tf.__version__)\n",
    "print ('All prerequisities imported.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d1e63-66e2-4bfa-b524-b443fa2917f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1\", url,\n",
    "                                  untar = True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "\n",
    "dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "print('dataset import and loading done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8ab09e-fce4-4891-9b12-53d5613e17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking directory contents\n",
    "os.listdir(dataset_dir)\n",
    "# you can also see this via terminal\n",
    "#shiva@ml-ai01:~/git/ML-AI/jupyternb$ ls aclImdb\n",
    "#imdbEr.txt  imdb.vocab  README  test  train\n",
    "#shiva@ml-ai01:~/git/ML-AI/jupyternb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df532f0-f015-435f-b9dd-c546ce87137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the training dir\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "print ('Training data dir = ', os.listdir(train_dir))\n",
    "print ('pos dir contains +ve review and neg dir contains -ve reviews')\n",
    "print ('the +ve, -ve distribution is even, so that weight is not lopsided on one classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e18dec-fd4e-43bd-952f-33cb9b5df64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The aclImdb/train/pos and aclImdb/train/neg directories contain many text files, each of which is a single movie review. \n",
    "# Let's take a look at one of them.\n",
    "sample_file = os.path.join(train_dir, 'pos/1181_9.txt')\n",
    "with open(sample_file) as f:\n",
    "    print('Title = ', f.name)\n",
    "    print('Review = ', f.read())\n",
    "\n",
    "print ('Sample review printed. The label +ve is the dir name, since this review comes from pos dir, this sentiment is +ve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059159eb-dd1e-47cf-87d9-d2df6c78550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the IMDB dataset contains additional folders, you will remove them before using this utility.\n",
    "remove_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(remove_dir)\n",
    "print ('unneeded directories removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1589730-45bb-4039-9a4e-ff382dbb6c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you will use the text_dataset_from_directory utility to create a labeled tf.data.Dataset. \n",
    "# tf.data is a powerful collection of tools for working with data.\n",
    "# When running a machine learning experiment, it is a best practice to divide your \n",
    "# dataset into three splits: train, validation, and test.\n",
    "# The IMDB dataset has already been divided into train and test, but it lacks a validation set.\n",
    "# Let's create a validation set using an 80:20 split of the training data by using the validation_split argument below.\n",
    "\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# Note: When using the validation_split and subset arguments, make sure to either specify a random seed, or \n",
    "# to pass shuffle=False, so that the validation and training splits have no overlap.\n",
    "\n",
    "raw_train_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f61553-f85b-43ab-a712-dc98c41916a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you're new to tf.data, you can also iterate over the dataset and print out a few examples as follows.\n",
    "\n",
    "for text_batch, label_batch in raw_train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print (\"Review\", text_batch.numpy()[i])\n",
    "        print(\"Label\", label_batch.numpy()[i])\n",
    "\n",
    "# clearly it's l,m,n,o,p,q .. i.e. n, neg = 0, and p, pos = 1 for classification purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492a082-1c87-4de4-adbe-5eb8448f2fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are 0 or 1. To see which of these correspond to positive and negative movie reviews, \n",
    "# you can check the class_names property on the dataset.\n",
    "\n",
    "print(\"Label 0 corresponds to \", raw_train_ds.class_names[0])\n",
    "print (\"Label 1 corresponds to \", raw_train_ds.class_names[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fe47b8-fd5d-4e37-a23e-29fa5cc6f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, you will create a validation and test dataset. You will use the remaining 5,000 reviews \n",
    "# from the training set for validation.\n",
    "\n",
    "# Note: When using the validation_split and subset arguments, make sure to either specify a random seed, or \n",
    "# to pass shuffle=False, so that the validation and training splits have no overlap.\n",
    "\n",
    "# when we did this the 1st time, the subset was training, here it is validation\n",
    "\n",
    "raw_val_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/train',\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=seed)\n",
    "print ('Done creating validation dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea61b80-2e2a-49c2-b796-f449c36fb8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the raw test dataset, notice, this is test, the training got split into training and validation\n",
    "# this is test (real) or untrained dataset .. the model has not seen this before.\n",
    "\n",
    "raw_test_ds = tf.keras.utils.text_dataset_from_directory(\n",
    "    'aclImdb/test',\n",
    "    batch_size=batch_size)\n",
    "\n",
    "print ('setting up raw_test_ds completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5ff0f-28b2-4925-a587-4126d67675ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, you will standardize, tokenize, and vectorize the data using the helpful tf.keras.layers.TextVectorization layer.\n",
    "# Standardization refers to preprocessing the text, typically to remove punctuation or HTML elements to simplify the dataset. \n",
    "# Tokenization refers to splitting strings into tokens (for example, splitting a sentence into individual words, \n",
    "# by splitting on whitespace). \n",
    "# Vectorization refers to converting tokens into numbers so they can be fed into a neural network. \n",
    "# All of these tasks can be accomplished with this layer.\n",
    "\n",
    "# this function just removes HTML\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_html,\n",
    "                                    '[%s]' % re.escape(string.punctuation),\n",
    "                                    '')\n",
    "\n",
    "print ('custom_standardization function is setup') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaedaef-b76e-4544-9255-bfd4319260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you will create a TextVectorization layer. You will use this layer to standardize, tokenize, and vectorize our data. \n",
    "# You set the output_mode to int to create unique integer indices for each token.\n",
    "\n",
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "print ('vectorized defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8be44b-f92f-4e02-a2f3-4a05bc3eae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, you will call adapt to fit the state of the preprocessing layer to the dataset. \n",
    "# This will cause the model to build an index of strings to integers.\n",
    "# Note: It's important to only use your training data when calling adapt (using the test set would leak information).\n",
    "\n",
    "# notice we are only calling train .. \n",
    "train_text = raw_train_ds.map(lambda x,y: x)\n",
    "vectorize_layer.adapt(train_text)\n",
    "\n",
    "print('text-only dataset created, no labels yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0676f844-98fc-45e3-8cfd-b7adca643fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a function to see the result of using this layer to preprocess some data.\n",
    "\n",
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "print ('vectorize_text function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67b5285-f456-4581-a213-ec67df21ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "# hmm .. code doesn't pull 32 reviews, just 1.\n",
    "\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review:\", first_review)\n",
    "print(\"Label:\", raw_test_ds.class_names[first_label])\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))\n",
    "\n",
    "# Review tf.Tensor(b'Great movie - especially the music - Etta James - \"At Last\"\n",
    "# interesting, their run pulls a different movie than my run ... why? if the files are \n",
    "# arranged in order, perhaps not? need to investigate later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ba78f6-6265-4e97-8972-5e3505bc31d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can lookup the token (string) that each integer corresponds to by calling .get_vocabulary() on the layer.\n",
    "# select numbers from the Vectorized View numbers\n",
    "\n",
    "print(\"1 --> \", vectorize_layer.get_vocabulary()[1])\n",
    "print(\"7 --> \", vectorize_layer.get_vocabulary()[7])\n",
    "print(\"4 --> \", vectorize_layer.get_vocabulary()[4])\n",
    "print(\"1233 --> \", vectorize_layer.get_vocabulary()[1233])\n",
    "print(\"1021 --> \", vectorize_layer.get_vocabulary()[1021])\n",
    "print(\"27 --> \", vectorize_layer.get_vocabulary()[27])\n",
    "\n",
    "print('Vocabulary size\" {}'.format(len(vectorize_layer.get_vocabulary())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23987b-1dcf-4060-b149-e0ca2eba18fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are nearly ready to train your model. As a final preprocessing step, you will apply the TextVectorization \n",
    "# layer you created earlier to the train, validation, and test dataset.\n",
    "\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "print('datasets are prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa917cd-8395-4a95-976b-cdf6117a8cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset for performance\n",
    "# These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
    "# .cache() keeps data in memory after it's loaded off disk. \n",
    "# This will ensure the dataset does not become a bottleneck while training your model. \n",
    "#If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "# .prefetch() overlaps data preprocessing and model execution while training.\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print ('configured datasets for performance using cache and prefetching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925f319-ffe1-4bc9-900f-5a59cc752a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's time to create your neural network:\n",
    "\n",
    "embedding_dim = 16\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Embedding(max_features, embedding_dim),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.GlobalAveragePooling1D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "print ('Model Summary = ', model.summary())\n",
    "print ('model is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36f3485-9d17-4bc4-87d1-ef7afd3e9444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets compile ...\n",
    "\n",
    "# A model needs a loss function and an optimizer for training. \n",
    "# Since this is a binary (+ve/1 or -ve/0) classification problem and the model outputs a probability \n",
    "# (a single-unit layer with a sigmoid activation), you'll use losses.BinaryCrossentropy loss function.\n",
    "\n",
    "model.compile(loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
    "\n",
    "print('model compiled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a194583-d44e-46c8-b7a5-b538cd30051f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets train ....\n",
    "\n",
    "epochs = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)\n",
    "\n",
    "print ('training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ac888-80dd-42b8-a459-d7d9254ef5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78e546-7d3b-4286-ad1e-de3e8dfd9662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of accuracy and loss over time\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "print ('\\nhistory data setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2514f1-396e-4672-8a15-29e5c5d5467b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# bo is for \"blue dots\"\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "# b is for \"solid blue line - Validation loss\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a496725d-fd11-44cd-a921-af9767e68766",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc61a1a9-614a-4ee1-8266-b83a26722558",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model = tf.keras.Sequential([\n",
    "  vectorize_layer,\n",
    "  model,\n",
    "  layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "export_model.compile(\n",
    "    loss=losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Test it with `raw_test_ds`, which yields raw strings\n",
    "loss, accuracy = export_model.evaluate(raw_test_ds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15958c2b-5924-42f9-86f5-7d87d847288f",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "  \"The movie was great!\",\n",
    "  \"The movie was okay.\",\n",
    "  \"The movie was terrible...\",\n",
    "  \"Best movie ever!\",\n",
    "  \"could not wait for the movie to end\",\n",
    "  \"What a waste of time with this move\",\n",
    "  \"The movie is fantastic\",\n",
    "  \"terrific, I'd watch it again\",\n",
    "    \"waste of time\"\n",
    "]\n",
    "\n",
    "prdct01 = export_model.predict(examples)\n",
    "print (prdct01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
